<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" >

<title>【第1周】深度学习基础 | 刘兴国的博客</title>

<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">

<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.2/css/all.css" integrity="sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr" crossorigin="anonymous">
<link rel="shortcut icon" href="https://liuxingguo9349.github.io/oucblog/favicon.ico?v=1691316527300">
<link rel="stylesheet" href="https://liuxingguo9349.github.io/oucblog/styles/main.css">


  
    <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css" />
  

  


<link rel="stylesheet" href="https://unpkg.com/aos@next/dist/aos.css" />
<script src="https://cdn.jsdelivr.net/npm/vue/dist/vue.js"></script>



    <meta name="description" content="1、PyTorch基础
PyTorch是一个python库，它主要提供了两个高级功能：

GPU加速的张量计算
构建在反向自动求导系统上的深度神经网络

1. 定义数据
一般定义数据使用torch.Tensor ， tensor的意思是张量..." />
    <meta name="keywords" content="" />
  </head>
  <body>
    <div id="app" class="main">

      <div class="sidebar" :class="{ 'full-height': menuVisible }">
  <div class="top-container" data-aos="fade-right">
    <div class="top-header-container">
      <a class="site-title-container" href="https://liuxingguo9349.github.io/oucblog">
        <img src="https://liuxingguo9349.github.io/oucblog/images/avatar.png?v=1691316527300" class="site-logo">
        <h1 class="site-title">刘兴国的博客</h1>
      </a>
      <div class="menu-btn" @click="menuVisible = !menuVisible">
        <div class="line"></div>
      </div>
    </div>
    <div>
      
        
          <a href="/oucblog/" class="site-nav">
            首页
          </a>
        
      
        
          <a href="/oucblog/archives" class="site-nav">
            归档
          </a>
        
      
        
          <a href="/oucblog/tags" class="site-nav">
            标签
          </a>
        
      
        
          <a href="/oucblog/post/about" class="site-nav">
            关于
          </a>
        
      
    </div>
  </div>
  <div class="bottom-container" data-aos="flip-up" data-aos-offset="0">
    <div class="social-container">
      
        
          <a class="social-link" href="https://github.com/liuxingguo9349" target="_blank">
            <i class="fab fa-github"></i>
          </a>
        
      
        
      
        
      
        
      
        
      
    </div>
    <div class="site-description">
      以梦为马，不负韶华
    </div>
    <div class="site-footer">
      Powered by <a href="https://github.com/getgridea/gridea" target="_blank">刘兴国的博客</a> | <a class="rss" href="https://liuxingguo9349.github.io/oucblog/atom.xml" target="_blank">RSS</a>
    </div>
  </div>
</div>


      <div class="main-container">
        <div class="content-container" data-aos="fade-up">
          <div class="post-detail">
            <h2 class="post-title">【第1周】深度学习基础</h2>
            <div class="post-date">2023-07-10</div>
            
            <div class="post-content" v-pre>
              <h1 id="1-pytorch基础"><strong>1、PyTorch基础</strong></h1>
<p>PyTorch是一个python库，它主要提供了两个高级功能：</p>
<ul>
<li>GPU加速的张量计算</li>
<li>构建在反向自动求导系统上的深度神经网络</li>
</ul>
<h2 id="1-定义数据">1. 定义数据</h2>
<p>一般定义数据使用torch.Tensor ， tensor的意思是张量，是数字各种形式的总称</p>
<pre><code class="language-python">import torch # 首先引入
x = torch.tensor() # num可以是数、向量、矩阵等张量
</code></pre>
<h2 id="2-定义操作">2. 定义操作</h2>
<p>凡是用Tensor进行各种运算的，都是Function<br>
最终，还是需要用Tensor来进行计算的，计算无非是<br>
基本运算，加减乘除，求幂求余<br>
布尔运算，大于小于，最大最小<br>
线性运算，矩阵乘法，求模，求行列式<br>
基本运算包括： abs/sqrt/div/exp/fmod/pow ，及一些三角函数 cos/ sin/ asin/ atan2/ cosh，及 ceil/round/floor/trunc 等具体在使用的时候可以百度一下<br>
布尔运算包括： gt/lt/ge/le/eq/ne，topk, sort, max/min<br>
线性计算包括： trace, diag, mm/bmm，t，dot/cross，inverse，svd 等</p>
<pre><code class="language-python"># 这里注意运算必须为同类型
# Scalar product
m @ v
</code></pre>
<p>原代码提供v不为浮点型，所以与m运算时会出错<br>
<img src="https://liuxingguo9349.github.io/oucblog/post-images/1688993308141.png" alt="" loading="lazy"></p>
<h1 id="2-螺旋数据分类"><strong>2、螺旋数据分类</strong></h1>
<pre><code class="language-python"># 引入plot_lib.py
!wget https://raw.githubusercontent.com/Atcold/pytorch-Deep-Learning/master/res/plot_lib.py
</code></pre>
<figure data-type="image" tabindex="1"><img src="https://liuxingguo9349.github.io/oucblog/post-images/1689041071120.png" alt="" loading="lazy"></figure>
<pre><code class="language-python">import random
import torch
from torch import nn, optim
import math
from IPython import display
from plot_lib import plot_data, plot_model, set_default

# 因为colab是支持GPU的，torch 将在 GPU 上运行
device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
print('device: ', device)

# 初始化随机数种子。神经网络的参数都是随机初始化的，
# 不同的初始化参数往往会导致不同的结果，当得到比较好的结果时我们通常希望这个结果是可以复现的，
# 因此，在pytorch中，通过设置随机数种子也可以达到这个目的
seed = 12345
random.seed(seed)
torch.manual_seed(seed)

N = 1000  # 每类样本的数量
D = 2  # 每个样本的特征维度
C = 3  # 样本的类别
H = 100  # 神经网络里隐层单元的数量
# 输出device:  cpu
</code></pre>
<pre><code class="language-python">X = torch.zeros(N * C, D).to(device)
Y = torch.zeros(N * C, dtype=torch.long).to(device)
for c in range(C):
    index = 0
    t = torch.linspace(0, 1, N) # 在[0，1]间均匀的取10000个数，赋给t
    # 下面的代码不用理解太多，总之是根据公式计算出三类样本（可以构成螺旋形）
    # torch.randn(N) 是得到 N 个均值为0，方差为 1 的一组随机数，注意要和 rand 区分开
    inner_var = torch.linspace( (2*math.pi/C)*c, (2*math.pi/C)*(2+c), N) + torch.randn(N) * 0.2

    # 每个样本的(x,y)坐标都保存在 X 里
    # Y 里存储的是样本的类别，分别为 [0, 1, 2]
    for ix in range(N * c, N * (c + 1)):
        X[ix] = t[index] * torch.FloatTensor((math.sin(inner_var[index]), math.cos(inner_var[index])))
        Y[ix] = c
        index += 1

print(&quot;Shapes:&quot;)
print(&quot;X:&quot;, X.size())
print(&quot;Y:&quot;, Y.size())
# 输出
Shapes:
X: torch.Size([3000, 2])
Y: torch.Size([3000])
</code></pre>
<pre><code class="language-python"># visualise the data
plot_data(X, Y)
</code></pre>
<figure data-type="image" tabindex="2"><img src="https://liuxingguo9349.github.io/oucblog/post-images/1689041514555.png" alt="" loading="lazy"></figure>
<h1 id="3-问题总结"><strong>3、问题总结</strong></h1>
<h2 id="1-alexnet有哪些特点为什么可以比lenet取得更好的性能">1. AlexNet有哪些特点？为什么可以比LeNet取得更好的性能？</h2>
<p>AlexNet是一种卷积神经网络（CNN）的架构，由Alex Krizhevsky与Ilya Sutskever和Geoffrey Hinton合作设计，后者是Krizhevsky的博士导师。AlexNet在2012年9月30日的ImageNet大规模视觉识别挑战赛中获胜，该网络的Top-5错误率为15.3%，比第二名低了10.8个百分点。原论文的主要结论是，模型的深度对于提高性能至关重要，但由于在训练过程中使用了图形处理器（GPU），使得计算具有可行性。<br>
AlexNet的特点有以下几点：</p>
<ul>
<li>AlexNet包含八层，前五层是卷积层，后三层是全连接层。网络除了最后一层外，分为两个副本，每个副本运行在一个GPU上。</li>
<li>AlexNet使用了非饱和的ReLU激活函数，显示出比tanh和sigmoid更好的训练性能。</li>
<li>AlexNet使用了局部响应归一化（LRN）层，以增强模型的泛化能力。</li>
<li>AlexNet使用了重叠的最大池化层，以减少特征图的大小，并提高平移不变性。</li>
<li>AlexNet使用了数据增强技术，如随机裁剪、水平翻转、颜色变换等，以扩大训练集并防止过拟合。</li>
<li>AlexNet使用了丢弃（dropout）技术，在全连接层中随机丢弃一些神经元，以减少参数数量并防止过拟合。</li>
<li></li>
</ul>
<p>AlexNet之所以可以比LeNet取得更好的性能，主要有以下几个原因：</p>
<ul>
<li>AlexNet比LeNet更深更宽，有更多的参数和特征表示能力。</li>
<li>AlexNet使用了GPU加速计算，使得训练更快，并可以处理更大的图像输入。</li>
<li>AlexNet使用了ImageNet数据集，该数据集包含了1000个类别和1400万张图像，比LeNet使用的MNIST数据集更复杂和多样化。</li>
<li>AlexNet使用了一些新颖的技术，如ReLU、LRN、dropout等，提高了模型的训练效率和泛化能力。</li>
</ul>
<h2 id="2-激活函数有哪些作用">2. 激活函数有哪些作用？</h2>
<p>激活函数的作用是给神经网络的输出加上非线性因素，使得神经网络可以拟合非线性的数据。激活函数可以增强神经网络的表达能力和学习能力，解决线性模型所不能解决的问题。激活函数还可以对数据进行归一化，将输入数据映射到某个范围内，防止数据过大导致的溢出风险。<br>
常见的激活函数有以下几种：</p>
<ul>
<li>Sigmoid函数：将实数映射到 (0,1) 的区间，可以用来做二分类。Sigmoid函数具有梯度平滑、输出归一化、可微分等优点，但也存在梯度消失、计算成本高、不以零为中心等缺点。</li>
<li>Tanh函数：将实数映射到 (-1,1) 的区间，是Sigmoid函数的放大和平移版本。Tanh函数比Sigmoid函数更接近零中心，但也会有梯度消失的问题。</li>
<li>ReLU函数：将负数置为零，保留正数不变。ReLU函数可以改善梯度消失问题，加速梯度下降的收敛速度，计算简单高效。但ReLU函数也有不可导、死亡神经元、输出不归一化等缺点。</li>
<li>Leaky ReLU函数：在ReLU函数的基础上，给负数部分加上一个很小的斜率，避免了死亡神经元的问题，但仍然存在不可导、输出不归一化等缺点。</li>
<li>Softmax函数：将一组实数映射到 (0,1) 的区间，并且使得它们的和为1。Softmax函数可以用来做多分类，它可以输出每个类别的概率分布。</li>
</ul>
<h2 id="3-梯度消失现象是什么">3. 梯度消失现象是什么？</h2>
<p>梯度消失现象是指在深度神经网络中，靠近输入层的参数的梯度因为连乘了很多小于1的数而变得非常小，接近于0，导致这些参数无法得到有效的更新。梯度消失会影响神经网络的训练效果和收敛速度，使得网络难以学习到复杂的特征。<br>
梯度消失的主要原因有以下几个：</p>
<ul>
<li>网络层数过深，导致反向传播时梯度以指数形式衰减。</li>
<li>激活函数的选择不当，例如sigmoid函数和tanh函数，在饱和区的导数接近于0，使得梯度无法有效传递。</li>
<li>参数初始化不合理，例如权重过小或过大，导致激活值落入饱和区或者方差过大。</li>
</ul>
<p>梯度消失的一些常见的解决方法有以下几个：</p>
<ul>
<li>选择合适的激活函数，例如ReLU函数和其变种，它们在正区间的导数为常数，可以避免梯度消失。</li>
<li>使用批量归一化（batch normalization）技术，可以规范化每一层的输出，防止数据分布的偏移和变化。</li>
<li>使用残差连接（residual connection）技术，可以在网络中添加跨层的直接连接，使得梯度可以直接从后面的层传到前面的层。</li>
<li>使用合理的参数初始化方法，例如Xavier初始化或He初始化，可以根据网络层数和输入输出维度自适应地调整权重的初始值。</li>
</ul>
<h2 id="4-神经网络是更宽好还是更深好">4. 神经网络是更宽好还是更深好？</h2>
<p>神经网络的宽度和深度是两个重要的因素，影响着网络的性能和泛化能力。一般来说，增加宽度或深度都可以提高网络的表达能力和拟合能力，但也会带来一些问题和挑战，比如计算复杂度、过拟合、梯度消失等。因此，神经网络的设计需要根据具体的任务和数据集进行权衡和调整。</p>
<h2 id="5-为什么要使用softmax">5. 为什么要使用Softmax？</h2>
<p>Softmax函数是一种激活函数，它可以将一个含任意实数的K维向量“压缩”到另一个K维实向量中，使得每一个元素的范围都在(0,1)之间，并且所有元素的和为1。Softmax函数可以将一个数值向量归一化为一个概率分布向量，且各个概率之和为1。<br>
Softmax函数的作用是：</p>
<ul>
<li>Softmax函数可以用来作为神经网络的最后一层，用于多分类问题的输出。Softmax函数可以预测每个类别的概率，并选择预测值最高的类别作为结果。例如，在手写数字识别问题中，Softmax函数可以输出0-9这10个数字的概率分布，并选择概率最大的数字作为识别结果。</li>
<li>Softmax函数常常和交叉熵损失函数一起结合使用，作为神经网络的优化目标。交叉熵损失函数可以衡量神经网络输出的概率分布与真实标签的概率分布之间的差异，越小说明越接近。通过反向传播算法，可以根据交叉熵损失函数对神经网络的参数进行更新，使得神经网络的输出更接近真实标签。</li>
</ul>
<h2 id="6-sgd-和-adam-哪个更有效">6. SGD 和 Adam 哪个更有效？</h2>
<p>SGD 和 Adam 都是常用的优化算法，但是它们的适用场景不同。SGD 适用于凸优化问题，而 Adam 适用于非凸优化问题。在训练深度学习模型时，Adam 通常比 SGD 更快地收敛，但是 SGD 可能会在某些情况下更好地泛化。如果数据集很大，那么 Adam 可能是更好的选择，因为它可以更快地收敛。如果数据集很小，则 SGD 可能是更好的选择，因为它可以更好地泛化。</p>

            </div>
            
            

            
              
                <div id="gitalk-container" data-aos="fade-in"></div>
              

              
            

          </div>

        </div>
      </div>
    </div>

    <script src="https://unpkg.com/aos@next/dist/aos.js"></script>
<script type="application/javascript">

AOS.init();

var app = new Vue({
  el: '#app',
  data: {
    menuVisible: false,
  },
})

</script>


  <script src="//cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.5.1/build/highlight.min.js"></script>
  <script>
    hljs.initHighlightingOnLoad()
  </script>




  
    <script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
    <script>

      var gitalk = new Gitalk({
        clientID: '079cee914f76656b4b21',
        clientSecret: 'bff979ffe33350b2717332f1f8459c0533f59a96',
        repo: 'oucblog',
        owner: 'liuxingguo9349',
        admin: ['liuxingguo9349'],
        id: (location.pathname).substring(0, 49),      // Ensure uniqueness and length less than 50
        distractionFreeMode: false  // Facebook-like distraction free mode
      })

      gitalk.render('gitalk-container')

    </script>
  

  




  </body>
</html>
